
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°† Markdown æ–‡æ¡£ä¸­çš„è¿œç¨‹å›¾ç‰‡ä¸‹è½½åˆ°æ–‡æ¡£åŒçº§ç›®å½•ä¸‹çš„ attachment å­æ–‡ä»¶å¤¹ï¼Œå¹¶æŠŠæ–‡æ¡£ä¸­çš„å›¾ç‰‡å¼•ç”¨æ”¹ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„ã€‚
- æ”¯æŒå¤„ç†å•ä¸ª .md æ–‡ä»¶ï¼Œæˆ–æŒ‡å®šæ–‡ä»¶å¤¹å†…ï¼ˆå¯é€’å½’ï¼‰æ‰€æœ‰ .md æ–‡ä»¶
- å…¼å®¹ Obsidianï¼ˆç›¸å¯¹è·¯å¾„å½¢å¼ï¼Œå¦‚ attachment/xxx.pngï¼‰
- å¤„ç†ä»¥ä¸‹å›¾ç‰‡å¼•ç”¨å½¢å¼ï¼š
  * Markdown å†…è”å›¾ç‰‡è¯­æ³•: ![alt](url "title")
  * HTML <img src="..."/> æ ‡ç­¾
  * Obsidian åµŒå…¥: ![[...]]ï¼ˆè‹¥ä¸º http/https åˆ™ä¸‹è½½å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼›è‹¥ä¸ºæœ¬åœ°åˆ™ä¿æŒï¼‰
  * Markdown å¼•ç”¨å¼å®šä¹‰: [id]: url "title"ï¼ˆä¼šæ›´æ–° url ä¸ºæœ¬åœ°è·¯å¾„ï¼‰
æ³¨æ„ï¼š
- å·²ä¸ºæœ¬åœ°æ–‡ä»¶ï¼ˆç›¸å¯¹è·¯å¾„å­˜åœ¨ï¼‰æˆ– data: / obsidian:// / file:// çš„å¼•ç”¨å°†è·³è¿‡
- å¦‚ä¸‹è½½æ–‡ä»¶åå†²çªï¼Œå°†è‡ªåŠ¨åŠ  (1), (2), ... åç¼€
"""

from __future__ import annotations

import argparse
import os
import re
import sys
import time
import json
from pathlib import Path
from typing import Dict, Optional, Tuple
from urllib.parse import urlparse, unquote
from urllib.request import Request, urlopen

ATTACH_DIR_NAME_DEFAULT = "attachment"
DEFAULT_TIMEOUT = 25
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) md-image-localizer/1.0"
ACCEPT_HEADER = "image/avif,image/webp,image/apng,image/*,*/*;q=0.8"

# å¸¸è§ Content-Type åˆ°æ‰©å±•åçš„æ˜ å°„
CONTENT_TYPE_TO_EXT = {
    "image/jpeg": ".jpg",
    "image/jpg": ".jpg",
    "image/png": ".png",
    "image/gif": ".gif",
    "image/webp": ".webp",
    "image/bmp": ".bmp",
    "image/svg+xml": ".svg",
    "image/tiff": ".tiff",
    "image/x-icon": ".ico",
    "image/heic": ".heic",
}

# æ”¯æŒçš„å›¾ç‰‡æ‰©å±•åé›†åˆ
IMAGE_EXTS = {
    ".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp", ".svg", ".tif", ".tiff", ".ico", ".heic",
}

# æ­£åˆ™æ¨¡å¼
MD_IMAGE_RE = re.compile(r"!\[([^\]]*)\]\(((?:[^()\\]|\\.|(?:\([^()]*\)))+)\)")
HTML_IMG_RE = re.compile(r'(<img\b[^>]*\bsrc=["\'])([^"\']+)(["\'][^>]*>)', re.IGNORECASE)
WIKILINK_EMBED_RE = re.compile(r"!\[\[(.*?)\]\]")
# Markdown å¼•ç”¨å¼å®šä¹‰ï¼Œå¦‚: [id]: https://example.com/img.png "title"
REF_DEF_RE = re.compile(r'^\s*\[([^\]]+)\]:\s*(\S+)(?:\s+(".*?"|\'.*?\'|\(.*?\)))?\s*$', re.MULTILINE)


def is_remote_url(url: str) -> bool:
    low = url.strip().lower()
    return low.startswith("http://") or low.startswith("https://")


def is_skippable_scheme(url: str) -> bool:
    low = url.strip().lower()
    return (
        low.startswith("data:")
        or low.startswith("obsidian://")
        or low.startswith("file://")
    )


def guess_ext_from_content_type(content_type: Optional[str]) -> Optional[str]:
    if not content_type:
        return None
    ct = content_type.split(";")[0].strip().lower()
    return CONTENT_TYPE_TO_EXT.get(ct)


def sanitize_filename(name: str) -> str:
    # Windows ä¸å…è®¸çš„å­—ç¬¦: \ / : * ? " < > |
    forbidden = '\\/:*?"<>|'
    safe = "".join(ch for ch in name if ch not in forbidden)
    # æ¸…ç†æ§åˆ¶å­—ç¬¦å’Œå°¾éƒ¨ç©ºæ ¼/ç‚¹ï¼ˆWindowsï¼‰
    safe = "".join(ch for ch in safe if ch.isprintable())
    safe = safe.strip(" .")
    return safe or "image"


def ensure_unique_path(dest_dir: Path, filename: str) -> Path:
    base = Path(filename).stem
    ext = Path(filename).suffix
    candidate = dest_dir / (base + ext)
    idx = 1
    while candidate.exists():
        candidate = dest_dir / f"{base} ({idx}){ext}"
        idx += 1
    return candidate


def extract_filename_from_url(url: str) -> Tuple[str, Optional[str]]:
    """
    ä» URL æå–åŸå§‹æ–‡ä»¶åï¼ˆå»é™¤æŸ¥è¯¢/ç‰‡æ®µï¼‰ï¼Œè¿”å› (basename_without_query, ext_if_any)
    """
    parsed = urlparse(url)
    path = parsed.path or ""
    name = os.path.basename(path)
    name = unquote(name)
    # å»é™¤å¯èƒ½æ®‹ç•™çš„éæ³•å­—ç¬¦
    name = sanitize_filename(name)
    base, ext = os.path.splitext(name)
    return (name if name else "image", ext if ext else None)


def read_text_with_fallback(path: Path) -> str:
    encodings = ["utf-8", "utf-16", "gb18030"]
    for enc in encodings:
        try:
            return path.read_text(encoding=enc)
        except UnicodeDecodeError:
            continue
        except OSError:
            break
    return path.read_text(encoding="utf-8", errors="ignore")


def write_text_utf8(path: Path, text: str) -> None:
    path.write_text(text, encoding="utf-8", newline="\n")


def split_md_target(raw: str) -> Tuple[str, str]:
    """
    è§£æ Markdown () å†…çš„ targetï¼Œæ‹†å‡º url ä¸å°¾éš title/å‚æ•°ç‰‡æ®µ
    ä¾‹å¦‚ï¼š
      'https://a/b.png "title"' -> ('https://a/b.png', ' "title"')
      '<https://a/b.png> "t"' -> ('https://a/b.png', ' "t"')
    """
    s = raw.strip()
    # å°–æ‹¬å·åŒ…è£¹çš„ URLï¼Œä¾‹å¦‚ <https://a/b.png> "title"
    if s.startswith("<") and ">" in s:
        end = s.find(">")
        url = s[1:end].strip()
        trailing = s[end + 1 :].rstrip()
        return url, (" " + trailing) if trailing else ""
    # éå°–æ‹¬å·ï¼Œå–ç¬¬ä¸€ä¸ªç©ºç™½å‰ä¸º URL
    parts = s.split()
    if not parts:
        return "", ""
    url = parts[0].strip()
    trailing = s[len(parts[0]) :].rstrip()
    return url, trailing if trailing.startswith(" ") else ((" " + trailing) if trailing else "")


def download_image(
    url: str,
    dest_dir: Path,
    timeout: int,
    preferred_basename: Optional[str] = None,
    ext_hint: Optional[str] = None,
    retries: int = 2,
    retry_delay: float = 1.2,
) -> Optional[Path]:
    """
    ä¸‹è½½å›¾ç‰‡åˆ° dest_dirï¼Œè¿”å›æœ€ç»ˆä¿å­˜çš„ Pathï¼ˆå”¯ä¸€æ–‡ä»¶åï¼‰ã€‚å¤±è´¥è¿”å› Noneã€‚
    æ”¯æŒé‡è¯•ä¸é€€é¿ï¼›ç»Ÿä¸€ UA/Accept å¤´ï¼›æŒ‰ Content-Type çŒœæ‰©å±•ã€‚
    """
    dest_dir.mkdir(parents=True, exist_ok=True)
    last_err: Optional[Exception] = None
    for attempt in range(retries + 1):
        try:
            req = Request(url, headers={"User-Agent": USER_AGENT, "Accept": ACCEPT_HEADER})
            with urlopen(req, timeout=timeout) as resp:
                content = resp.read()
                content_type = resp.headers.get("Content-Type", "")

                # æ–‡ä»¶åä¸æ‰©å±•åå¤„ç†
                raw_name, ext_from_url = extract_filename_from_url(url)
                ext = None
                if ext_hint and ext_hint.startswith("."):
                    ext = ext_hint
                elif ext_from_url:
                    ext = ext_from_url
                else:
                    guessed = guess_ext_from_content_type(content_type)
                    if guessed:
                        ext = guessed
                # å¦‚æœæœ€ç»ˆä»æ— æ‰©å±•åï¼Œç»™ä¸ªé»˜è®¤ .img
                if not ext:
                    ext = ".img"

                base = preferred_basename if preferred_basename else sanitize_filename(os.path.splitext(raw_name)[0])
                base = base.strip(" .")
                filename = f"{base}{ext}"
                final_path = ensure_unique_path(dest_dir, filename)
                final_path.write_bytes(content)
                if attempt > 0:
                    print(f"â„¹ï¸ é‡è¯•æˆåŠŸï¼š{url}")
                return final_path
        except Exception as e:
            last_err = e
            if attempt < retries:
                try:
                    time.sleep(retry_delay)
                except Exception:
                    pass
            else:
                print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url} -> {e}")
    return None


class FileProcessor:
    def __init__(
        self,
        md_path: Path,
        attach_dir_name: str,
        timeout: int,
        dry_run: bool,
        rename_images: bool = False,
        rename_strategy: str = "context",
        max_name_len: int = 80,
        retry: int = 2,
        retry_delay: float = 1.2,
    ):
        self.md_path = md_path
        self.md_dir = md_path.parent
        self.attach_dir = self.md_dir / attach_dir_name
        self.timeout = timeout
        self.dry_run = dry_run
        self.rename_images = rename_images
        self.rename_strategy = rename_strategy
        self.max_name_len = max_name_len
        self.retry = retry
        self.retry_delay = retry_delay
        # ç›¸åŒ URL åœ¨åŒä¸€æ–‡ä»¶å†…é‡å¤å‡ºç°æ—¶å…±ç”¨ä¸€æ¬¡ä¸‹è½½
        self.url_cache: Dict[str, Path] = {}
        # å¤„ç†æ—¶ä¸Šä¸‹æ–‡
        self.current_text: str = ""
        self.doc_title: Optional[str] = None
        self.image_seq: int = 0
        # ä½ç½®ä¸å‘½åçŠ¶æ€ï¼ˆç”¨äºâ€œä¸Šä¸€å›¾åˆ°å½“å‰å›¾ä¹‹é—´çš„æ–‡å­—â€å›¾æ„æå–ï¼‰
        self.last_image_pos: int = 0
        self.last_intent: Optional[str] = None
        self.block_index: int = 0
        self.block_image_index: int = 0
        # ç›‘æ§æ•°æ®
        self.remote_expected: int = 0
        self.remaining_remote: list[Dict] = []

    def is_local_existing(self, src: str) -> bool:
        # ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼ˆä½äºåº“å†…ï¼‰æ˜¯å¦å·²å­˜åœ¨
        if is_remote_url(src) or is_skippable_scheme(src):
            return False
        # è§„èŒƒåŒ–åˆ°æ–‡ä»¶æ‰€åœ¨ç›®å½•
        try:
            candidate = (self.md_dir / Path(src)).resolve()
        except Exception:
            return False
        return candidate.exists()

    def url_to_local_rel(
        self,
        url: str,
        *,
        alt: Optional[str] = None,
        alias: Optional[str] = None,
        trailing_title: Optional[str] = None,
        html_tag: Optional[str] = None,
        match_pos: Optional[int] = None,
    ) -> str:
        """
        å°†è¿œç¨‹ URL ä¸‹è½½ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆä»¥ md æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸ºåŸºå‡†ï¼Œposix åˆ†éš”ï¼‰ã€‚
        ä¼šæ ¹æ®é…ç½®é€‰æ‹©æ˜¯å¦é‡å‘½åå›¾ç‰‡æ–‡ä»¶ã€‚
        """
        if url in self.url_cache:
            local_path = self.url_cache[url]
        else:
            preferred_basename: Optional[str] = None
            # æ‰©å±•åæç¤ºæ¥è‡ª URL
            _, ext_from_url = extract_filename_from_url(url)
            ext_hint = ext_from_url

            if self.rename_images:
                if not self.doc_title:
                    self.doc_title = self._extract_doc_title(self.current_text)
                preferred_basename = self._suggest_image_basename(
                    url=url,
                    alt=alt,
                    alias=alias,
                    trailing_title=trailing_title,
                    html_tag=html_tag,
                    match_pos=match_pos,
                )

            if self.dry_run:
                # é¢„è§ˆæ¨¡å¼ï¼šæ„é€ ä¸€ä¸ªé¢„æœŸçš„æ–‡ä»¶åï¼ˆä¸è½ç›˜ï¼‰
                base = preferred_basename if preferred_basename else sanitize_filename(os.path.splitext(extract_filename_from_url(url)[0])[0])
                base = base[: self.max_name_len].strip(" .")
                ext = ext_hint if ext_hint else ".img"
                if not ext.startswith("."):
                    ext = "." + ext
                filename = f"{base}{ext}"
                local_path = self.attach_dir / filename
            else:
                # å®é™…ä¸‹è½½ï¼Œå¸¦é‡è¯•ä¸å®¹é”™
                try:
                    local_path_opt = download_image(
                        url,
                        self.attach_dir,
                        self.timeout,
                        preferred_basename=preferred_basename[: self.max_name_len] if preferred_basename else None,
                        ext_hint=ext_hint,
                        retries=self.retry,
                        retry_delay=self.retry_delay,
                    )
                    if local_path_opt is None:
                        # ä¸‹è½½å¤±è´¥ï¼šè¿”å›åŸå§‹ urlï¼ˆä¸æ”¹å†™ï¼Œä¸çº³å…¥ç¼“å­˜/è®¡æ•°ï¼‰
                        return url
                    local_path = local_path_opt
                    # ä»…åœ¨å®é™…ä¸‹è½½æˆåŠŸæ—¶å¢åŠ åºå·å¹¶ç¼“å­˜
                    self.image_seq += 1
                    self.url_cache[url] = local_path
                except Exception as e:
                    print(f"âš ï¸ ä¸‹è½½å¼‚å¸¸ï¼š{url} -> {e}")
                    return url

        rel = os.path.relpath(local_path, self.md_dir).replace("\\", "/")
        return rel

    def relocate_or_rename_local(
        self,
        src: str,
        *,
        alt: Optional[str] = None,
        alias: Optional[str] = None,
        trailing_title: Optional[str] = None,
        html_tag: Optional[str] = None,
        match_pos: Optional[int] = None,
    ) -> str:
        """
        å°†å·²æœ‰æœ¬åœ°å›¾ç‰‡ç§»åŠ¨/é‡å‘½ååˆ° attachment å¹¶è¿”å›æ–°çš„ç›¸å¯¹è·¯å¾„ã€‚
        - è‹¥å·²åœ¨ attachment ä¸”æ–‡ä»¶åç¬¦åˆæœŸæœ›ï¼Œåˆ™ä¿æŒä¸å˜
        - dry-run æ¨¡å¼ä¸‹ä»…è¿”å›é¢„æœŸè·¯å¾„ï¼Œä¸å®é™…å˜æ›´
        """
        # è§£ææœ¬åœ°è·¯å¾„
        try:
            src_path = (self.md_dir / Path(src)).resolve()
        except Exception:
            return src
        if not src_path.exists():
            return src

        # æ‰©å±•å
        ext = src_path.suffix
        if not ext:
            ext = ".img"
        if not ext.startswith("."):
            ext = "." + ext

        # ç›®æ ‡åŸºæœ¬å
        if self.rename_images:
            if not self.doc_title:
                self.doc_title = self._extract_doc_title(self.current_text)
            base = self._suggest_image_basename(
                url=str(src_path),
                alt=alt,
                alias=alias,
                trailing_title=trailing_title,
                html_tag=html_tag,
                match_pos=match_pos,
            )
        else:
            base = sanitize_filename(src_path.stem)

        base = base[: self.max_name_len].strip(" .")
        filename = f"{base}{ext}"
        dest_dir = self.attach_dir

        if self.dry_run:
            dest_path = dest_dir / filename
            return os.path.relpath(dest_path, self.md_dir).replace("\\", "/")

        # å®é™…é‡å‘½å/æ¬ç§»
        dest_dir.mkdir(parents=True, exist_ok=True)
        dest_path = ensure_unique_path(dest_dir, filename)

        try:
            if src_path == dest_path:
                pass
            elif src_path.parent == dest_dir:
                src_path.rename(dest_path)
            else:
                # è‹¥ä¸åœ¨ attachmentï¼Œåˆ™å¤åˆ¶åˆ° attachmentï¼ˆé¿å…ç ´åå¤–éƒ¨åŸå§‹èµ„æºï¼‰
                dest_path.write_bytes(src_path.read_bytes())
            self.image_seq += 1
        except Exception:
            # å¤±è´¥åˆ™è¿”å›åŸå§‹ç›¸å¯¹è·¯å¾„
            return os.path.relpath(src_path, self.md_dir).replace("\\", "/")

        return os.path.relpath(dest_path, self.md_dir).replace("\\", "/")

    def _extract_doc_title(self, text: str) -> str:
        """
        æå–æ–‡æ¡£æ ‡é¢˜ï¼š
        - ä¼˜å…ˆ YAML frontmatter ä¸­çš„ parent æˆ– title
        - å¦åˆ™å–ç¬¬ä¸€æ¡ Markdown æ ‡é¢˜è¡Œï¼ˆ# æˆ– ##ï¼‰
        - å†å¦åˆ™ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        # YAML frontmatter
        m = re.match(r"^---\s*(.*?)\s*---", text, flags=re.DOTALL)
        if m:
            fm = m.group(1)
            # ç®€å•æŸ¥æ‰¾ parent/title å­—æ®µ
            for key in ["parent", "title", "Parent", "Title"]:
                km = re.search(rf"^\s*{key}\s*:\s*(.+)$", fm, flags=re.MULTILINE)
                if km:
                    candidate = km.group(1).strip().strip("'\"")
                    if candidate:
                        return sanitize_filename(candidate)
        # Markdown æ ‡é¢˜
        for line in text.splitlines():
            l = line.strip()
            if l.startswith("#"):
                ttl = l.lstrip("#").strip()
                if ttl:
                    return sanitize_filename(ttl)
        # æ–‡ä»¶å
        return sanitize_filename(self.md_path.stem)

    def _tokenize_keywords(self, s: str) -> list[str]:
        """
        æå–ä¸Šä¸‹æ–‡å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰ï¼Œå»é‡ä¿åºã€‚
        """
        if not s:
            return []
        chinese = re.findall(r"[\u4e00-\u9fff]{2,}", s)
        english = re.findall(r"[A-Za-z]{4,}", s)
        tokens = chinese + english
        seen = set()
        unique = []
        for t in tokens:
            if t not in seen:
                seen.add(t)
                unique.append(t)
        return unique

    def _clean_title_fragment(self, s: Optional[str]) -> str:
        if not s:
            return ""
        s = s.strip()
        if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
            s = s[1:-1]
        if s.startswith("(") and s.endswith(")"):
            s = s[1:-1]
        return s.strip()

    def _suggest_image_basename(
        self,
        url: str,
        alt: Optional[str],
        alias: Optional[str],
        trailing_title: Optional[str],
        html_tag: Optional[str],
        match_pos: Optional[int],
    ) -> str:
        """
        ä¾æ®é‡å‘½åç­–ç•¥ç”Ÿæˆå›¾ç‰‡åŸºåã€‚
        - seqï¼šæ–‡ä»¶æ ‡é¢˜ + ä¸¤ä½å‡ºç°åºå·ï¼ˆç¤ºä¾‹ï¼šæ ‡é¢˜_01ï¼‰
        - contextï¼šæ®µè½æ‘˜è¦ + å›¾æ„ç¼–å·ï¼ˆç¤ºä¾‹ï¼šæ ‡é¢˜_æ®µè½æ‘˜è¦_å›¾æ„01ï¼‰
        - simple/semanticï¼šæ²¿ç”¨ context ç”Ÿæˆé€»è¾‘
        """
        idx = self.image_seq + 1  # é¢„ä¼°åºå·ï¼ˆä¸‹è½½åä¼šè‡ªå¢ï¼‰
        doc_title = self.doc_title or self._extract_doc_title(self.current_text)

        def _strip_trailing_image_ext(s: str) -> str:
            if not s:
                return s
            s2 = re.sub(r"(?i)(?:[._\-\s])?(?:png|jpe?g|gif|webp|bmp|svg|tiff?|ico|heic)$", "", s)
            return s2.rstrip(" ._")

        if (self.rename_strategy or "").lower() == "seq":
            # å›ºå®šæ ¼å¼ï¼š<æ–‡æ¡£æ ‡é¢˜> + ä¸¤ä½å…¨å±€åºå·ï¼ˆç¤ºä¾‹ï¼šæ ‡é¢˜_01ï¼‰
            base = f"{doc_title}_{idx:02d}"
            base = sanitize_filename(base)
            base = _strip_trailing_image_ext(base)
            if len(base) > self.max_name_len:
                base = base[: self.max_name_len].rstrip(" ._")
            return base

        # é»˜è®¤/å…¶å®ƒç­–ç•¥ï¼šåŸâ€œæ®µè½æ‘˜è¦ + å›¾æ„ç¼–å·â€
        context_desc = self._analyze_paragraph_context(match_pos)
        if context_desc:
            clean_desc = re.sub(r'[^\w\u4e00-\u9fff]+', '_', context_desc).strip('_')
            base = f"{doc_title}_{clean_desc}_å›¾æ„{idx}"
        else:
            base = f"{doc_title}_å›¾æ„{idx}"
        base = sanitize_filename(base)
        base = _strip_trailing_image_ext(base)
        if len(base) > self.max_name_len:
            base = base[: self.max_name_len].rstrip(" ._")
        return base

    def _analyze_paragraph_context(self, match_pos: Optional[int]) -> str:
        """
        åˆ†æå›¾ç‰‡æ‰€åœ¨æ®µè½çš„ä¸Šä¸‹æ–‡ï¼Œè¿”å›ç®€æ´çš„æ®µè½å†…å®¹æ€»ç»“ã€‚
        """
        if match_pos is None or not self.current_text:
            return ""

        lines = self.current_text.splitlines()
        if match_pos >= len(self.current_text):
            return ""

        # æ‰¾åˆ°å›¾ç‰‡æ‰€åœ¨çš„è¡Œ
        current_line_idx = 0
        current_pos = 0
        for i, line in enumerate(lines):
            if current_pos <= match_pos < current_pos + len(line) + 1:  # +1 for newline
                current_line_idx = i
                break
            current_pos += len(line) + 1

        # å‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„éç©ºæ®µè½ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        paragraph_lines = []
        for i in range(current_line_idx, -1, -1):
            line = lines[i].strip()
            if not line:
                if paragraph_lines:  # é‡åˆ°ç©ºè¡Œä¸”å·²æœ‰å†…å®¹ï¼Œåœæ­¢
                    break
                continue

            # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆä»¥#å¼€å¤´æˆ–å…¨å¤§å†™æ ‡é¢˜ï¼‰
            if line.startswith('#') or (line.isupper() and len(line) < 50):
                if not paragraph_lines:  # å¦‚æœè¿˜æ²¡æ‰¾åˆ°å†…å®¹ï¼Œç»§ç»­æ‰¾
                    continue
                break

            paragraph_lines.insert(0, line)

            # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼ˆè¶…è¿‡100å­—ç¬¦ï¼‰ï¼Œåœæ­¢
            if sum(len(l) for l in paragraph_lines) > 100:
                break

        # å‘ä¸‹æŸ¥æ‰¾è¡¥å……å†…å®¹
        for i in range(current_line_idx + 1, len(lines)):
            line = lines[i].strip()
            if not line:
                break

            if line.startswith('#') or (line.isupper() and len(line) < 50):
                break

            paragraph_lines.append(line)

            if sum(len(l) for l in paragraph_lines) > 200:
                break

        # åˆå¹¶æ®µè½å†…å®¹
        paragraph_text = ' '.join(paragraph_lines)

        # æå–å…³é”®è¯å¹¶ç”Ÿæˆç®€æ´æè¿°
        return self._summarize_paragraph(paragraph_text)

    def _summarize_paragraph(self, text: str) -> str:
        """
        ä»æ®µè½æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ç”Ÿæˆç®€æ´æè¿°ã€‚
        - å…ˆæ¸…ç†å›¾ç‰‡è¯­æ³•/æ ‡ç­¾ä¸è£¸éœ²çš„å›¾ç‰‡æ–‡ä»¶åï¼Œé¿å…æŠŠâ€œpng/jpgâ€ç­‰å™ªå£°æ··å…¥æ‘˜è¦
        """
        if not text.strip():
            return ""

        # 1) æ¸…ç† HTML æ ‡ç­¾ä¸ Markdown/Obsidian å›¾ç‰‡è¯­æ³•
        try:
            text2 = re.sub(r"<[^>]+>", "", text)
        except Exception:
            text2 = text
        try:
            text2 = MD_IMAGE_RE.sub("", text2)
        except Exception:
            pass
        try:
            text2 = WIKILINK_EMBED_RE.sub("", text2)
        except Exception:
            pass
        # 2) æ¸…ç†è£¸éœ²çš„å›¾ç‰‡é“¾æ¥/æ–‡ä»¶åï¼ˆ*.png/jpg/...ï¼‰
        text2 = re.sub(r"(?i)\b\S+\.(?:png|jpe?g|gif|webp|bmp|svg|tiff?|ico|heic)\b", "", text2)

        # 3) ç§»é™¤å¸¸è§çš„æ–‡ç« æ ‡è®°å’Œå¤šä½™ç¬¦å·
        text2 = re.sub(r'[ã€ã€ã€Œã€ã€ã€‘ã€Šã€‹()ï¼ˆï¼‰\*\-\+\=\[\]{}|\\]', '', text2)

        # 4) åˆ†å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text2)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return ""

        # 5) ä¼˜å…ˆé€‰æ‹©åŒ…å«å…³é”®ç”Ÿç‰©å­¦æœ¯è¯­çš„å¥å­
        key_terms = ['åŒå£³çº²', 'èˆ¹è›†', 'å·¨å‹èˆ¹è›†', 'è¶³ä¸', 'é³ƒ', 'å£³', 'é’»æœ¨', 'ä¹ æ€§', 'å¤–è§‚', 'ç»“æ„', 'ç…§ç‰‡', 'ç¤ºæ„å›¾']

        selected_sentences = []
        for sentence in sentences:
            if any(term in sentence for term in key_terms):
                selected_sentences.append(sentence)
                if len(selected_sentences) >= 2:  # æœ€å¤šé€‰2å¥
                    break

        if not selected_sentences:
            selected_sentences = sentences[:2]

        # 6) åˆå¹¶å¹¶æˆªæ–­
        summary = ' '.join(selected_sentences)
        if len(summary) > 50:
            summary = summary[:47] + "..."
        return summary.strip()

    def _derive_intent_between(self, prev_pos: int, curr_pos: int) -> str:
        """
        æå–â€œä¸Šä¸€å¼ å›¾ç‰‡åˆ°å½“å‰å›¾ç‰‡ä¹‹é—´â€çš„æ–‡æœ¬æ„å›¾æ‘˜è¦ã€‚
        - è¿‡æ»¤æ‰è¿‡çŸ­æˆ–åªæœ‰æ ‡ç‚¹/ç©ºç™½çš„ç‰‡æ®µ
        - ä½¿ç”¨ç°æœ‰æ®µè½æ‘˜è¦å™¨ç”Ÿæˆç®€æ´å›¾æ„
        """
        if not self.current_text:
            return ""
        try:
            raw = self.current_text[prev_pos:curr_pos]
        except Exception:
            return ""
        snippet = re.sub(r"\s+", " ", raw).strip()
        # è‡³å°‘åŒ…å«è‹¥å¹²å¯è§å­—ç¬¦ï¼ˆä¸­æ–‡/è‹±æ–‡/æ•°å­—ï¼‰
        visible_chars = re.findall(r"[\u4e00-\u9fffA-Za-z0-9]", snippet)
        if len(visible_chars) < 4:
            return ""
        return self._summarize_paragraph(snippet)

    def _update_intent_and_counters(self, match_pos: Optional[int]) -> tuple[str, int, int]:
        """
        æ ¹æ®å½“å‰ä½ç½®æ›´æ–°å›¾æ„ä¸è®¡æ•°å™¨ï¼š
        - è‹¥æœ‰æ–°å›¾æ„ï¼šå—åºå· +1ï¼Œå—å†…å›¾ç‰‡åºå·ç½® 1ï¼Œæ›´æ–° last_intent
        - è‹¥æ— æ–°æ–‡å­—ä½†å·²æœ‰ last_intentï¼šæ²¿ç”¨å›¾æ„ï¼Œå—å†…å›¾ç‰‡åºå· +1
        - è‹¥æ— ä»»ä½•å›¾æ„ï¼šä½¿ç”¨å ä½â€œå›¾æ„â€ï¼Œåˆå§‹åŒ–å—åºä¸åºå·
        è¿”å› (intent_str, block_index, block_image_index)
        """
        curr = match_pos or 0
        intent = self._derive_intent_between(self.last_image_pos, curr)
        if intent:
            clean_intent = re.sub(r'[^\w\u4e00-\u9fff]+', '_', intent).strip('_')
            if self.last_intent != clean_intent:
                self.block_index += 1
                self.block_image_index = 1
                self.last_intent = clean_intent
            else:
                self.block_image_index += 1
            self.last_image_pos = curr
            return self.last_intent or "", self.block_index, self.block_image_index

        # æ— å®é™…æ–‡å­—ï¼šæ²¿ç”¨ä¸Šä¸€å›¾æ„ï¼ˆè‹¥æœ‰ï¼‰
        if self.last_intent:
            self.block_image_index += 1
            self.last_image_pos = curr
            return self.last_intent, self.block_index, self.block_image_index

        # é¦–æ¬¡ä¸”æ²¡æœ‰å¯æå–çš„å›¾æ„ï¼šåˆå§‹åŒ–ä¸ºå ä½å—
        if self.block_index == 0:
            self.block_index = 1
        self.block_image_index = max(self.block_image_index, 1)
        self.last_image_pos = curr
        return "", self.block_index, self.block_image_index

    def replace_md_inline(self, m: re.Match) -> str:
        alt = m.group(1)
        raw_target = m.group(2)
        url, trailing = split_md_target(raw_target)
        if not url or is_skippable_scheme(url):
            return m.group(0)

        # ä» trailing ä¸­å°è¯•æå– "title"
        title_match = re.search(r'(".*?"|\'.*?\')', trailing or "")
        title = None
        if title_match:
            title = title_match.group(0)

        def _clean_alt(a: Optional[str]) -> str:
            alt_raw = a or ""
            alt_clean = re.sub(r"<[^>]+>", "", alt_raw)
            alt_clean = alt_clean.replace("|", " ").strip()
            alt_clean = re.sub(r"\s+", " ", alt_clean).strip()
            return alt_clean

        def _title_trailing(t: Optional[str]) -> str:
            ttext = (t or "").strip().strip('"').strip("'")
            return f' "{ttext}"' if ttext else ""

        if is_remote_url(url):
            new_rel = self.url_to_local_rel(
                url,
                alt=alt if alt else None,
                trailing_title=title,
                match_pos=m.start(),
            )
            return f"![{_clean_alt(alt)}]({new_rel}{_title_trailing(title)})"
        else:
            # æœ¬åœ°å›¾ç‰‡ï¼šå¦‚å¯ç”¨é‡å‘½åï¼Œåˆ™è¿›è¡Œé‡å‘½å/æ¬ç§»å¹¶æ›´æ–°é“¾æ¥
            if self.rename_images and self.is_local_existing(url):
                new_rel = self.relocate_or_rename_local(
                    url,
                    alt=alt if alt else None,
                    trailing_title=title,
                    match_pos=m.start(),
                )
                return f"![{_clean_alt(alt)}]({new_rel}{_title_trailing(title)})"
            return m.group(0)

    def replace_html_img(self, m: re.Match) -> str:
        head, src, tail = m.groups()
        if not src or is_skippable_scheme(src):
            return m.group(0)

        full_tag = f"{head}{src}{tail}"
        alt_attr = None
        alt_m = re.search(r'\balt=["\']([^"\']+)["\']', full_tag, flags=re.IGNORECASE)
        if alt_m:
            alt_attr = alt_m.group(1)

        if is_remote_url(src):
            new_rel = self.url_to_local_rel(
                src,
                alt=alt_attr,
                html_tag=full_tag,
                match_pos=m.start(),
            )
            return f'{head}{new_rel}{tail}'
        else:
            # æœ¬åœ°å›¾ç‰‡ï¼šå¦‚å¯ç”¨é‡å‘½åï¼Œåˆ™è¿›è¡Œé‡å‘½å/æ¬ç§»å¹¶æ›´æ–°é“¾æ¥
            if self.rename_images and self.is_local_existing(src):
                new_rel = self.relocate_or_rename_local(
                    src,
                    alt=alt_attr,
                    html_tag=full_tag,
                    match_pos=m.start(),
                )
                return f'{head}{new_rel}{tail}'
            return m.group(0)

    def replace_wikilink_embed(self, m: re.Match) -> str:
        inside = m.group(1).strip()
        # å¯èƒ½å¸¦åˆ«åï¼Œå¦‚ target|alias
        if "|" in inside:
            target, alias = inside.split("|", 1)
            target = target.strip()
            alias = alias.strip()
        else:
            target, alias = inside, None

        if not target or is_skippable_scheme(target):
            return m.group(0)
        if is_remote_url(target):
            new_rel = self.url_to_local_rel(
                target,
                alias=alias,
                match_pos=m.start(),
            )
            return f"![[{new_rel}|{alias}]]" if alias else f"![[{new_rel}]]"
        else:
            # æœ¬åœ°å›¾ç‰‡çš„ wikilinkï¼šä»…å¯¹å›¾ç‰‡æ‰©å±•åè¿›è¡Œé‡å‘½åå¤„ç†
            suffix = Path(target).suffix.lower()
            if self.rename_images and suffix in IMAGE_EXTS and self.is_local_existing(target):
                new_rel = self.relocate_or_rename_local(
                    target,
                    alias=alias,
                    match_pos=m.start(),
                )
                return f"![[{new_rel}|{alias}]]" if alias else f"![[{new_rel}]]"
            return m.group(0)

    def replace_ref_defs(self, text: str) -> str:
        """
        å¤„ç†å¼•ç”¨å¼å›¾ç‰‡/é“¾æ¥å®šä¹‰ï¼ŒæŠŠè¿œç¨‹ URL ä¸‹è½½åˆ°æœ¬åœ°å¹¶æ”¹å†™ä¸ºæœ¬åœ°è·¯å¾„ã€‚
        """
        def _rep(m: re.Match) -> str:
            key = m.group(1)
            url = m.group(2)
            title = m.group(3) or ""
            if url and is_remote_url(url) and not is_skippable_scheme(url):
                new_rel = self.url_to_local_rel(url)
                return f"[{key}]: {new_rel}{(' ' + title) if title else ''}"
            return m.group(0)
        return REF_DEF_RE.sub(_rep, text)

    def process(self) -> Tuple[int, int, int]:
        """
        å¤„ç†å•ä¸ª md æ–‡ä»¶ï¼Œè¿”å› (ä¸‹è½½æ•°, æ›¿æ¢æ•°, å¼•ç”¨å¼å®šä¹‰æ›¿æ¢æ•°)
        """
        original = read_text_with_fallback(self.md_path)
        text = original

        # ç»Ÿè®¡é¢„æœŸè¿œç¨‹å›¾ç‰‡æ•°é‡ï¼ˆç”¨äºäºŒæ¬¡æ ¡éªŒï¼‰
        remote_expected = 0
        try:
            # Markdown å†…è”
            for m in MD_IMAGE_RE.finditer(original):
                url, _ = split_md_target(m.group(2))
                if url and is_remote_url(url) and not is_skippable_scheme(url):
                    remote_expected += 1
            # HTML <img>
            for m in HTML_IMG_RE.finditer(original):
                src = m.group(2)
                if src and is_remote_url(src) and not is_skippable_scheme(src):
                    remote_expected += 1
            # Obsidian åµŒå…¥
            for m in WIKILINK_EMBED_RE.finditer(original):
                inside = m.group(1).strip()
                tgt = inside.split("|", 1)[0].strip()
                if tgt and is_remote_url(tgt) and not is_skippable_scheme(tgt):
                    remote_expected += 1
        except Exception:
            remote_expected = 0
        # è®°å½•åˆ°å®ä¾‹ï¼Œä¾¿äºäºŒæ¬¡æ ¡éªŒä¸æŠ¥å‘Š
        self.remote_expected = remote_expected

        # ä¸ºæ›¿æ¢é˜¶æ®µå‡†å¤‡ä¸Šä¸‹æ–‡
        self.current_text = original
        self.doc_title = self._extract_doc_title(original)
        self.image_seq = 0
        # åˆå§‹åŒ–å›¾æ„/åˆ†ç»„è®¡æ•°å™¨
        self.last_image_pos = 0
        self.last_intent = None
        self.block_index = 0
        self.block_image_index = 0
        self.url_cache.clear()

        # å…ˆå¤„ç†å¼•ç”¨å¼å®šä¹‰
        before = text
        text = self.replace_ref_defs(text)
        ref_repl = 0 if text == before else len(list(REF_DEF_RE.finditer(before)))  # è¿‘ä¼¼

        # å¤„ç† Markdown å†…è”
        before = text
        text = MD_IMAGE_RE.sub(self.replace_md_inline, text)
        inline_repl = 0 if text == before else 1  # ç»Ÿè®¡ç²—ç•¥ä¸ºæ˜¯å¦å‘ç”Ÿå˜åŒ–

        # å¤„ç† HTML <img>
        before = text
        text = HTML_IMG_RE.sub(self.replace_html_img, text)
        html_repl = 0 if text == before else 1

        # å¤„ç† Obsidian åµŒå…¥
        before = text
        text = WIKILINK_EMBED_RE.sub(self.replace_wikilink_embed, text)
        embed_repl = 0 if text == before else 1

        # ä¸‹è½½æ€»æ•° = å®é™…ç¼“å­˜çš„è¿œç¨‹ url ä¸ªæ•°ï¼ˆdry-run ä¸º 0ï¼‰
        download_count = 0 if self.dry_run else len(self.url_cache)

        # æ‰«æå‰©ä½™è¿œç¨‹å¼•ç”¨ï¼ˆè¡Œå·/ç±»å‹/URLï¼‰ï¼Œç”¨äºæ ¸éªŒæŠ¥å‘Š
        remaining: list[Dict] = []
        try:
            # MD å†…è”
            for m in MD_IMAGE_RE.finditer(text):
                url, _ = split_md_target(m.group(2))
                if url and is_remote_url(url) and not is_skippable_scheme(url):
                    remaining.append({"kind": "md", "url": url, "line": text[:m.start()].count("\n") + 1})
            # HTML <img>
            for m in HTML_IMG_RE.finditer(text):
                src = m.group(2)
                if src and is_remote_url(src) and not is_skippable_scheme(src):
                    remaining.append({"kind": "html", "url": src, "line": text[:m.start()].count("\n") + 1})
            # Obsidian åµŒå…¥
            for m in WIKILINK_EMBED_RE.finditer(text):
                inside = m.group(1).strip()
                tgt = inside.split("|", 1)[0].strip()
                if tgt and is_remote_url(tgt) and not is_skippable_scheme(tgt):
                    remaining.append({"kind": "wikilink", "url": tgt, "line": text[:m.start()].count("\n") + 1})
        except Exception:
            remaining = []
        self.remaining_remote = remaining

        # äºŒæ¬¡æ ¡éªŒä¸æç¤ºï¼ˆä»…åœ¨é dry-run ä¸‹ï¼‰
        if not self.dry_run and self.remote_expected > 0 and download_count < self.remote_expected:
            try:
                print(f"âš ï¸ è¿œç¨‹å›¾ç‰‡ä¸‹è½½ä¸å®Œå…¨ï¼šé¢„æœŸ {self.remote_expected}ï¼Œå®é™…ä¸‹è½½ {download_count}ã€‚å‰©ä½™è¿œç¨‹ {len(self.remaining_remote)} å¤„ã€‚")
                for r in self.remaining_remote[:10]:
                    print(f"   â€¢ [{r['kind']}] line {r['line']}: {r['url']}")
                if len(self.remaining_remote) > 10:
                    print(f"   â€¢ å…¶ä½™ {len(self.remaining_remote) - 10} å¤„å·²çœç•¥â€¦")
            except Exception:
                pass

        # å¦‚å†…å®¹å˜åŒ–åˆ™å†™å›
        if text != original and not self.dry_run:
            write_text_utf8(self.md_path, text)

        replace_total = (inline_repl + html_repl + embed_repl)
        return download_count, replace_total, ref_repl


def find_md_files(target: Path, recursive: bool) -> list[Path]:
    results: list[Path] = []
    if target.is_file() and target.suffix.lower() == ".md":
        return [target.resolve()]
    if target.is_dir():
        if recursive:
            for p in target.rglob("*.md"):
                if p.is_file():
                    results.append(p.resolve())
        else:
            for p in target.glob("*.md"):
                if p.is_file():
                    results.append(p.resolve())
    return sorted(results)

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Download remote images referenced in Markdown into a local 'attachment' folder next to each Markdown file, then rewrite references to local relative paths (Obsidian-compatible).",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("path", type=Path, help="Path to a Markdown file or a folder containing Markdown files")
    parser.add_argument("-r", "--recursive", action="store_true", help="Recursively process subfolders when 'path' is a folder")
    parser.add_argument("--attach-dir-name", default=ATTACH_DIR_NAME_DEFAULT, help="Attachment folder name to create next to each Markdown file")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT, help="HTTP timeout (seconds) when downloading images")
    parser.add_argument("--dry-run", action="store_true", help="Preview changes without downloading or modifying files")
    parser.add_argument("--rename-images", action="store_true", help="Rename downloaded images using document title and nearby context")
    parser.add_argument("--rename-strategy", choices=["simple", "context", "semantic", "seq"], default="seq", help="Strategy when renaming images (seq: æ ‡é¢˜+ä¸¤ä½å…¨å±€ç¼–å·ï¼Œå¦‚ {title}_{index:02d})")
    parser.add_argument("--max-name-len", type=int, default=80, help="Maximum base filename length when renaming images")
    parser.add_argument("--retry", type=int, default=2, help="Retry count for image downloads")
    parser.add_argument("--retry-delay", type=float, default=1.2, help="Delay (seconds) between retries")
    parser.add_argument("--report", type=Path, default=None, help="Write an aggregated JSON report of processing results")
    return parser

def main() -> None:
    try:
        sys.stdout.reconfigure(encoding="utf-8")
    except Exception:
        pass

    parser = build_parser()
    args = parser.parse_args()

    target: Path = args.path.expanduser()
    if not target.exists():
        print(f"âŒ Path not found: {target}")
        sys.exit(1)

    md_files = find_md_files(target, args.recursive)
    if not md_files:
        print("âš ï¸ No Markdown files found to process.")
        sys.exit(0)

    total_would_dl = 0
    total_repl = 0
    total_ref = 0
    processed = 0

    print(f"ğŸ” Found {len(md_files)} Markdown file(s). {'[dry-run]' if args.dry_run else ''}")
    reports: list[Dict] = []
    for md in md_files:
        try:
            processor = FileProcessor(
                md,
                args.attach_dir_name,
                args.timeout,
                args.dry_run,
                args.rename_images,
                args.rename_strategy,
                args.max_name_len,
                retry=args.retry,
                retry_delay=args.retry_delay,
            )
            dl, repl, ref = processor.process()
            processed += 1
            total_would_dl += len(processor.url_cache)
            total_repl += repl
            total_ref += ref
            rel_md = os.path.relpath(md, Path.cwd())
            if args.dry_run:
                print(f"  â€¢ {rel_md} -> would download {len(processor.url_cache)} image(s), replace {repl} block(s), update {ref} reference(s)")
            else:
                print(f"  â€¢ {rel_md} -> downloaded {dl} image(s), replaced {repl} block(s), updated {ref} reference(s)")
                # æ™ºèƒ½æ ¸éªŒï¼šå‰©ä½™è¿œç¨‹å¼•ç”¨é€æ¡åˆ—å‡ºï¼ˆæœ€å¤š 10 æ¡ï¼‰
                if getattr(processor, "remaining_remote", []):
                    print(f"    Remaining remote refs ({len(processor.remaining_remote)}):")
                    for r in processor.remaining_remote[:10]:
                        print(f"      - [{r.get('kind')}] line {r.get('line')}: {r.get('url')}")
                    if len(processor.remaining_remote) > 10:
                        print(f"      - ... {len(processor.remaining_remote) - 10} more")
            # æ±‡æ€»æŠ¥å‘Š
            reports.append({
                "md": str(md),
                "downloaded": dl,
                "replaced_blocks": repl,
                "updated_ref_defs": ref,
                "remote_expected": getattr(processor, "remote_expected", 0),
                "remaining_remote_count": len(getattr(processor, "remaining_remote", [])),
                "remaining_remote": getattr(processor, "remaining_remote", []),
            })
        except Exception as e:
            print(f"  â€¢ {md} -> Error: {e}")

    print("â€”â€”")
    if args.dry_run:
        print(f"âœ… Dry-run complete. Processed {processed} file(s). Would download {total_would_dl} image(s). Would replace {total_repl} block(s). Would update {total_ref} reference definition(s).")
    else:
        print(f"âœ… Done. Processed {processed} file(s). Downloaded {total_would_dl} image(s). Replaced {total_repl} block(s). Updated {total_ref} reference definition(s).")
        # å†™æŠ¥å‘Šï¼ˆå¦‚æŒ‡å®šï¼‰
        if args.report:
            try:
                report_path = args.report.expanduser().resolve()
                report_path.parent.mkdir(parents=True, exist_ok=True)
                report_path.write_text(json.dumps(reports, ensure_ascii=False, indent=2), encoding="utf-8")
                print(f"ğŸ“ Report written: {report_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to write report: {e}")

if __name__ == "__main__":
    main()
